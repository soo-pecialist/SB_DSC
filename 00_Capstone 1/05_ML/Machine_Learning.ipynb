{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import matplotlib.cm as cm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "import gc\n",
    "import string\n",
    "from textblob import Word, TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score, roc_auc_score\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.utils.fixes import signature\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.multiclass import OneVsOneClassifier, OneVsRestClassifier\n",
    "\n",
    "\n",
    "# from pyspark import SparkContext\n",
    "# from spark_sklearn.util import createLocalSparkSession\n",
    "# from pyspark.serializers import BatchedSerializer, PickleSerializer\n",
    "# from spark_sklearn import GridSearchCV\n",
    "# from pyspark import broadcast\n",
    "# from zodbpickle import pickle\n",
    "\n",
    "# Setup Seaborn\n",
    "sns.set(style='whitegrid', font_scale=0.8, rc={\"lines.linewidth\":2, 'grid.color': '.8', 'grid.linestyle':':'})\n",
    "# Set up Matplotlib\n",
    "matplotlib.rc('xtick', labelsize=12)     \n",
    "matplotlib.rc('ytick', labelsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read dataframe from story telling procedure\n",
    "df_train = pd.read_csv('./data/df_train_v4.csv.gz', compression='gzip', parse_dates=['reviewTime'], low_memory=False)\n",
    "df_test = pd.read_csv('./data/df_test_v4.csv.gz', compression='gzip', parse_dates=['reviewTime'], low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>categories</th>\n",
       "      <th>helpful_num</th>\n",
       "      <th>review</th>\n",
       "      <th>polarity</th>\n",
       "      <th>word_count</th>\n",
       "      <th>word_density</th>\n",
       "      <th>helpfulness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A3DMPV6RWRHPM3</td>\n",
       "      <td>0306813920</td>\n",
       "      <td>2005-11-30</td>\n",
       "      <td>Books</td>\n",
       "      <td>26</td>\n",
       "      <td>Informative, but unbalanced.. This bio gave me...</td>\n",
       "      <td>0.256913</td>\n",
       "      <td>353</td>\n",
       "      <td>5.779661</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AZYJ9TS07B02W</td>\n",
       "      <td>B000O178BY</td>\n",
       "      <td>2007-04-19</td>\n",
       "      <td>CDs &amp; Vinyl</td>\n",
       "      <td>8</td>\n",
       "      <td>Boogie Apocalypse. Yeow, what a disappointment...</td>\n",
       "      <td>-0.016427</td>\n",
       "      <td>189</td>\n",
       "      <td>6.021053</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A18U7ZLAA90PNM</td>\n",
       "      <td>B004TLH6HQ</td>\n",
       "      <td>2011-06-07</td>\n",
       "      <td>Electronics</td>\n",
       "      <td>26</td>\n",
       "      <td>False advertising - Does NOT fit IPad 2. I hav...</td>\n",
       "      <td>-0.022692</td>\n",
       "      <td>154</td>\n",
       "      <td>5.148387</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID        asin reviewTime   categories  helpful_num  \\\n",
       "0  A3DMPV6RWRHPM3  0306813920 2005-11-30        Books           26   \n",
       "1   AZYJ9TS07B02W  B000O178BY 2007-04-19  CDs & Vinyl            8   \n",
       "2  A18U7ZLAA90PNM  B004TLH6HQ 2011-06-07  Electronics           26   \n",
       "\n",
       "                                              review  polarity  word_count  \\\n",
       "0  Informative, but unbalanced.. This bio gave me...  0.256913         353   \n",
       "1  Boogie Apocalypse. Yeow, what a disappointment... -0.016427         189   \n",
       "2  False advertising - Does NOT fit IPad 2. I hav... -0.022692         154   \n",
       "\n",
       "   word_density  helpfulness  \n",
       "0      5.779661          5.0  \n",
       "1      6.021053          2.0  \n",
       "2      5.148387          5.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>categories</th>\n",
       "      <th>helpful_num</th>\n",
       "      <th>review</th>\n",
       "      <th>polarity</th>\n",
       "      <th>word_count</th>\n",
       "      <th>word_density</th>\n",
       "      <th>helpfulness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AQRVDI6DFSZ86</td>\n",
       "      <td>B001MYIXAC</td>\n",
       "      <td>2009-04-02</td>\n",
       "      <td>Movies &amp; TV</td>\n",
       "      <td>27</td>\n",
       "      <td>Great Film, Great Transfer, BOGUS SUBTITLES!. ...</td>\n",
       "      <td>0.173042</td>\n",
       "      <td>252</td>\n",
       "      <td>6.351779</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A1248JRIINWRTH</td>\n",
       "      <td>B000NQ95H0</td>\n",
       "      <td>2009-10-10</td>\n",
       "      <td>Home &amp; Kitchen</td>\n",
       "      <td>10</td>\n",
       "      <td>Terrific quality for a great price. I was a li...</td>\n",
       "      <td>0.156119</td>\n",
       "      <td>192</td>\n",
       "      <td>5.673575</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A2F540P3L6P5CL</td>\n",
       "      <td>0547074239</td>\n",
       "      <td>2008-08-31</td>\n",
       "      <td>Books</td>\n",
       "      <td>9</td>\n",
       "      <td>In need of some serious overhaul prior to publ...</td>\n",
       "      <td>-0.007879</td>\n",
       "      <td>197</td>\n",
       "      <td>5.474747</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID        asin reviewTime      categories  helpful_num  \\\n",
       "0   AQRVDI6DFSZ86  B001MYIXAC 2009-04-02     Movies & TV           27   \n",
       "1  A1248JRIINWRTH  B000NQ95H0 2009-10-10  Home & Kitchen           10   \n",
       "2  A2F540P3L6P5CL  0547074239 2008-08-31           Books            9   \n",
       "\n",
       "                                              review  polarity  word_count  \\\n",
       "0  Great Film, Great Transfer, BOGUS SUBTITLES!. ...  0.173042         252   \n",
       "1  Terrific quality for a great price. I was a li...  0.156119         192   \n",
       "2  In need of some serious overhaul prior to publ... -0.007879         197   \n",
       "\n",
       "   word_density  helpfulness  \n",
       "0      6.351779          4.0  \n",
       "1      5.673575          5.0  \n",
       "2      5.474747          3.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((994036, 10), (248510, 10))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we did in EDA project, let's combine categories 'Electronics' and 'All Electronics' into one 'Electronics'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Merge All Electronics to Electronics\n",
    "df_train.categories = df_train.categories.replace({'All Electronics': 'Electronics'})\n",
    "df_test.categories = df_test.categories.replace({'All Electronics': 'Electronics'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tfidf features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## prepare stopwords to remove common 1000 frequent words\n",
    "text = df_train.review.str.cat(sep=\" \")\n",
    "stopwords_list = Counter(text.lower().split()).most_common(1000)\n",
    "stopwords_list = [item[0] for item in stopwords_list]\n",
    "stopwords_set = stopwords.words('english') + stopwords_list \\\n",
    "                + ['quot', 'Ye', 'dr', 'etc', 'mr', 'ye', 'yes', 'paul', 'oh', 'hes', 'shes',\n",
    "                  'b', 'c', 'de', 'f', 'ii', 'im', 'ive', 'la', 'michael', 'ms', 'miss', 'theres',\n",
    "                  'p', 'w', 'x', 'youll', 'youre', 'youd', 'cds', 'youve', 'ill', 'theyre',\n",
    "                  'theyd', 'therere', 'thats']\n",
    "stopwords_set = set([re.sub(\"[^a-zA-Z]\", \"\", word).lower() for word in stopwords_set]) - {\"\"}\n",
    "\n",
    "## read in already processed clean text dataframes\n",
    "X_train_review_clean = pd.read_csv('./data/X_train_review_clean.csv.gz', header=None, compression='gzip', low_memory=False)\n",
    "X_test_review_clean = pd.read_csv('./data/X_test_review_clean.csv.gz', header=None, compression='gzip', low_memory=False)\n",
    "\n",
    "## by default, pd.read_csv reads data in dataframe. Change it to pd.Series\n",
    "X_train_review_clean = X_train_review_clean[0]\n",
    "X_test_review_clean = X_test_review_clean[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Informative, but unbalanced.. This bio gave me...\n",
       "1    Boogie Apocalypse. Yeow, what a disappointment...\n",
       "2    False advertising - Does NOT fit IPad 2. I hav...\n",
       "Name: review, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.review.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    informative but unbalance this bio give me a p...\n",
       "1    boogie apocalypse yeow what a disappointment d...\n",
       "2    false advertise do not fit ipad i have a zierr...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_review_clean.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((994036, 500), <994036x500 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 9564638 stored elements in Compressed Sparse Row format>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## filter out stop words of English, and too frequently used words are out, \n",
    "## words used in less than 1% and more than 90% reviews excluded\n",
    "## After running several times this code block, max_df: 0.55~0.90 does not change vocabulary number \n",
    "## take top 500 frequent terms \n",
    "vectorizer = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(1,1), \n",
    "                             min_df=0.005, max_df=0.9, max_features=500, stop_words=stopwords_set)\n",
    "\n",
    "V_train = vectorizer.fit_transform(X_train_review_clean.astype('U'))\n",
    "V_test = vectorizer.transform(X_test_review_clean.astype('U'))\n",
    "\n",
    "## dimension and type check\n",
    "V_train.shape, V_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimension reduction with TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced 300-feature data holds 71.40 % variance of the original data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((994036, 300), (248510, 300))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Dimension reduction to 300\n",
    "svd = TruncatedSVD(n_components=300, n_iter=7, random_state=7979)\n",
    "svd.fit(V_train)\n",
    "\n",
    "print(\"Reduced 300-feature data holds {:.2f} % variance of the original data\".format(svd.explained_variance_ratio_.sum() * 100))\n",
    "\n",
    "## transform arrays\n",
    "V_train_new = svd.transform(V_train)\n",
    "V_test_new = svd.transform(V_test)\n",
    "\n",
    "## change to data frames\n",
    "columns = ['pc'+str(i) for i in range(1, 300+1)]\n",
    "V_train_new = pd.DataFrame(V_train_new, columns=columns)\n",
    "V_test_new = pd.DataFrame(V_test_new, columns=columns)\n",
    "\n",
    "## dimension check\n",
    "V_train_new.shape, V_test_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get dummy variables for categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((994036, 38), (248510, 38))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## get dummy variabels for categories\n",
    "train_cat = pd.get_dummies(df_train.categories, prefix='cat')\n",
    "test_cat = pd.get_dummies(df_test.categories, prefix='cat')\n",
    "\n",
    "## drop original review, categories columns and concatenate dummy variables\n",
    "df_train = pd.concat([df_train.drop(columns=['categories']), train_cat], axis=1)\n",
    "df_test = pd.concat([df_test.drop(columns=['categories']), test_cat], axis=1)\n",
    "\n",
    "## check dimension\n",
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put it all together!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((994036, 337), (248510, 337))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## index reset for V_new dataframes\n",
    "V_train_new.index = df_train.index\n",
    "V_test_new.index = df_test.index\n",
    "\n",
    "## Combine all data frames \n",
    "df_train = pd.concat([df_train.drop(columns=['review']), V_train_new], axis=1)\n",
    "df_test = pd.concat([df_test.drop(columns=['review']), V_test_new], axis=1)\n",
    "\n",
    "## check dimension\n",
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>helpful_num</th>\n",
       "      <th>polarity</th>\n",
       "      <th>word_count</th>\n",
       "      <th>word_density</th>\n",
       "      <th>helpfulness</th>\n",
       "      <th>cat_Appliances</th>\n",
       "      <th>cat_Apps for Android</th>\n",
       "      <th>...</th>\n",
       "      <th>pc291</th>\n",
       "      <th>pc292</th>\n",
       "      <th>pc293</th>\n",
       "      <th>pc294</th>\n",
       "      <th>pc295</th>\n",
       "      <th>pc296</th>\n",
       "      <th>pc297</th>\n",
       "      <th>pc298</th>\n",
       "      <th>pc299</th>\n",
       "      <th>pc300</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A3DMPV6RWRHPM3</td>\n",
       "      <td>0306813920</td>\n",
       "      <td>2005-11-30</td>\n",
       "      <td>26</td>\n",
       "      <td>0.256913</td>\n",
       "      <td>353</td>\n",
       "      <td>5.779661</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015590</td>\n",
       "      <td>0.017478</td>\n",
       "      <td>0.010573</td>\n",
       "      <td>0.059862</td>\n",
       "      <td>0.051233</td>\n",
       "      <td>-0.004555</td>\n",
       "      <td>0.009048</td>\n",
       "      <td>-0.062087</td>\n",
       "      <td>0.054748</td>\n",
       "      <td>0.001599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AZYJ9TS07B02W</td>\n",
       "      <td>B000O178BY</td>\n",
       "      <td>2007-04-19</td>\n",
       "      <td>8</td>\n",
       "      <td>-0.016427</td>\n",
       "      <td>189</td>\n",
       "      <td>6.021053</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.059889</td>\n",
       "      <td>0.011316</td>\n",
       "      <td>0.009066</td>\n",
       "      <td>0.057939</td>\n",
       "      <td>-0.019484</td>\n",
       "      <td>-0.015208</td>\n",
       "      <td>-0.003141</td>\n",
       "      <td>0.037956</td>\n",
       "      <td>0.041262</td>\n",
       "      <td>-0.108097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A18U7ZLAA90PNM</td>\n",
       "      <td>B004TLH6HQ</td>\n",
       "      <td>2011-06-07</td>\n",
       "      <td>26</td>\n",
       "      <td>-0.022692</td>\n",
       "      <td>154</td>\n",
       "      <td>5.148387</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007588</td>\n",
       "      <td>-0.003663</td>\n",
       "      <td>-0.010679</td>\n",
       "      <td>-0.002417</td>\n",
       "      <td>0.009921</td>\n",
       "      <td>0.001906</td>\n",
       "      <td>0.008329</td>\n",
       "      <td>-0.007434</td>\n",
       "      <td>-0.005938</td>\n",
       "      <td>-0.007134</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 337 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID        asin reviewTime  helpful_num  polarity  word_count  \\\n",
       "0  A3DMPV6RWRHPM3  0306813920 2005-11-30           26  0.256913         353   \n",
       "1   AZYJ9TS07B02W  B000O178BY 2007-04-19            8 -0.016427         189   \n",
       "2  A18U7ZLAA90PNM  B004TLH6HQ 2011-06-07           26 -0.022692         154   \n",
       "\n",
       "   word_density  helpfulness  cat_Appliances  cat_Apps for Android    ...     \\\n",
       "0      5.779661          5.0               0                     0    ...      \n",
       "1      6.021053          2.0               0                     0    ...      \n",
       "2      5.148387          5.0               0                     0    ...      \n",
       "\n",
       "      pc291     pc292     pc293     pc294     pc295     pc296     pc297  \\\n",
       "0  0.015590  0.017478  0.010573  0.059862  0.051233 -0.004555  0.009048   \n",
       "1 -0.059889  0.011316  0.009066  0.057939 -0.019484 -0.015208 -0.003141   \n",
       "2 -0.007588 -0.003663 -0.010679 -0.002417  0.009921  0.001906  0.008329   \n",
       "\n",
       "      pc298     pc299     pc300  \n",
       "0 -0.062087  0.054748  0.001599  \n",
       "1  0.037956  0.041262 -0.108097  \n",
       "2 -0.007434 -0.005938 -0.007134  \n",
       "\n",
       "[3 rows x 337 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(994036, 333) (248510, 333) (994036,) (248510,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>helpful_num</th>\n",
       "      <th>polarity</th>\n",
       "      <th>word_count</th>\n",
       "      <th>word_density</th>\n",
       "      <th>cat_Appliances</th>\n",
       "      <th>cat_Apps for Android</th>\n",
       "      <th>cat_Arts, Crafts &amp; Sewing</th>\n",
       "      <th>cat_Automotive</th>\n",
       "      <th>cat_Baby</th>\n",
       "      <th>cat_Baby Products</th>\n",
       "      <th>...</th>\n",
       "      <th>pc291</th>\n",
       "      <th>pc292</th>\n",
       "      <th>pc293</th>\n",
       "      <th>pc294</th>\n",
       "      <th>pc295</th>\n",
       "      <th>pc296</th>\n",
       "      <th>pc297</th>\n",
       "      <th>pc298</th>\n",
       "      <th>pc299</th>\n",
       "      <th>pc300</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>9.940360e+05</td>\n",
       "      <td>9.940360e+05</td>\n",
       "      <td>9.940360e+05</td>\n",
       "      <td>9.940360e+05</td>\n",
       "      <td>9.940360e+05</td>\n",
       "      <td>9.940360e+05</td>\n",
       "      <td>9.940360e+05</td>\n",
       "      <td>9.940360e+05</td>\n",
       "      <td>9.940360e+05</td>\n",
       "      <td>9.940360e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>9.940360e+05</td>\n",
       "      <td>9.940360e+05</td>\n",
       "      <td>9.940360e+05</td>\n",
       "      <td>9.940360e+05</td>\n",
       "      <td>9.940360e+05</td>\n",
       "      <td>9.940360e+05</td>\n",
       "      <td>9.940360e+05</td>\n",
       "      <td>9.940360e+05</td>\n",
       "      <td>9.940360e+05</td>\n",
       "      <td>9.940360e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.178760e-16</td>\n",
       "      <td>-2.166002e-16</td>\n",
       "      <td>-5.021578e-17</td>\n",
       "      <td>-3.793256e-15</td>\n",
       "      <td>-1.448696e-14</td>\n",
       "      <td>8.757220e-15</td>\n",
       "      <td>1.027800e-14</td>\n",
       "      <td>7.586651e-15</td>\n",
       "      <td>-5.933334e-15</td>\n",
       "      <td>-1.120757e-14</td>\n",
       "      <td>...</td>\n",
       "      <td>3.026219e-16</td>\n",
       "      <td>5.180992e-16</td>\n",
       "      <td>4.679954e-17</td>\n",
       "      <td>-2.088562e-17</td>\n",
       "      <td>2.880885e-16</td>\n",
       "      <td>-1.476404e-17</td>\n",
       "      <td>7.776454e-16</td>\n",
       "      <td>-2.265700e-17</td>\n",
       "      <td>3.130718e-16</td>\n",
       "      <td>-1.905817e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-3.858956e-01</td>\n",
       "      <td>-6.107773e+00</td>\n",
       "      <td>-8.906790e-01</td>\n",
       "      <td>-7.708145e+00</td>\n",
       "      <td>-3.248693e-02</td>\n",
       "      <td>-2.016406e-02</td>\n",
       "      <td>-6.454477e-02</td>\n",
       "      <td>-7.705351e-02</td>\n",
       "      <td>-8.598270e-02</td>\n",
       "      <td>-2.807702e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.644358e+00</td>\n",
       "      <td>-6.188888e+00</td>\n",
       "      <td>-6.504232e+00</td>\n",
       "      <td>-6.626838e+00</td>\n",
       "      <td>-6.048934e+00</td>\n",
       "      <td>-7.119678e+00</td>\n",
       "      <td>-6.373493e+00</td>\n",
       "      <td>-6.540614e+00</td>\n",
       "      <td>-6.784550e+00</td>\n",
       "      <td>-6.452385e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-2.427416e-01</td>\n",
       "      <td>-5.158984e-01</td>\n",
       "      <td>-5.810408e-01</td>\n",
       "      <td>-6.267710e-01</td>\n",
       "      <td>-3.248693e-02</td>\n",
       "      <td>-2.016406e-02</td>\n",
       "      <td>-6.454477e-02</td>\n",
       "      <td>-7.705351e-02</td>\n",
       "      <td>-8.598270e-02</td>\n",
       "      <td>-2.807702e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.323144e-01</td>\n",
       "      <td>-5.327463e-01</td>\n",
       "      <td>-5.069046e-01</td>\n",
       "      <td>-5.008649e-01</td>\n",
       "      <td>-5.092769e-01</td>\n",
       "      <td>-4.788405e-01</td>\n",
       "      <td>-4.910171e-01</td>\n",
       "      <td>-4.966676e-01</td>\n",
       "      <td>-4.884351e-01</td>\n",
       "      <td>-4.919479e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-1.711645e-01</td>\n",
       "      <td>-4.528107e-04</td>\n",
       "      <td>-3.058068e-01</td>\n",
       "      <td>-4.992810e-02</td>\n",
       "      <td>-3.248693e-02</td>\n",
       "      <td>-2.016406e-02</td>\n",
       "      <td>-6.454477e-02</td>\n",
       "      <td>-7.705351e-02</td>\n",
       "      <td>-8.598270e-02</td>\n",
       "      <td>-2.807702e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.044235e-03</td>\n",
       "      <td>-1.560881e-03</td>\n",
       "      <td>5.647878e-03</td>\n",
       "      <td>9.869426e-04</td>\n",
       "      <td>-1.238488e-03</td>\n",
       "      <td>-2.720205e-03</td>\n",
       "      <td>2.391959e-02</td>\n",
       "      <td>-9.617627e-03</td>\n",
       "      <td>8.231765e-03</td>\n",
       "      <td>-1.890337e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>-1.011621e-02</td>\n",
       "      <td>5.287835e-01</td>\n",
       "      <td>2.145574e-01</td>\n",
       "      <td>5.737718e-01</td>\n",
       "      <td>-3.248693e-02</td>\n",
       "      <td>-2.016406e-02</td>\n",
       "      <td>-6.454477e-02</td>\n",
       "      <td>-7.705351e-02</td>\n",
       "      <td>-8.598270e-02</td>\n",
       "      <td>-2.807702e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>5.098305e-01</td>\n",
       "      <td>5.175950e-01</td>\n",
       "      <td>4.920822e-01</td>\n",
       "      <td>5.112307e-01</td>\n",
       "      <td>4.852234e-01</td>\n",
       "      <td>4.881347e-01</td>\n",
       "      <td>4.884094e-01</td>\n",
       "      <td>4.719028e-01</td>\n",
       "      <td>4.680939e-01</td>\n",
       "      <td>4.490253e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.450049e+02</td>\n",
       "      <td>4.478493e+00</td>\n",
       "      <td>2.551028e+01</td>\n",
       "      <td>1.526134e+02</td>\n",
       "      <td>3.078161e+01</td>\n",
       "      <td>4.959320e+01</td>\n",
       "      <td>1.549312e+01</td>\n",
       "      <td>1.297799e+01</td>\n",
       "      <td>1.163025e+01</td>\n",
       "      <td>3.561632e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>7.861697e+00</td>\n",
       "      <td>7.501955e+00</td>\n",
       "      <td>7.736336e+00</td>\n",
       "      <td>6.750965e+00</td>\n",
       "      <td>7.614505e+00</td>\n",
       "      <td>7.589073e+00</td>\n",
       "      <td>7.803906e+00</td>\n",
       "      <td>6.582658e+00</td>\n",
       "      <td>7.929147e+00</td>\n",
       "      <td>9.044717e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 333 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        helpful_num      polarity    word_count  word_density  cat_Appliances  \\\n",
       "count  9.940360e+05  9.940360e+05  9.940360e+05  9.940360e+05    9.940360e+05   \n",
       "mean   5.178760e-16 -2.166002e-16 -5.021578e-17 -3.793256e-15   -1.448696e-14   \n",
       "std    1.000001e+00  1.000001e+00  1.000001e+00  1.000001e+00    1.000001e+00   \n",
       "min   -3.858956e-01 -6.107773e+00 -8.906790e-01 -7.708145e+00   -3.248693e-02   \n",
       "25%   -2.427416e-01 -5.158984e-01 -5.810408e-01 -6.267710e-01   -3.248693e-02   \n",
       "50%   -1.711645e-01 -4.528107e-04 -3.058068e-01 -4.992810e-02   -3.248693e-02   \n",
       "75%   -1.011621e-02  5.287835e-01  2.145574e-01  5.737718e-01   -3.248693e-02   \n",
       "max    1.450049e+02  4.478493e+00  2.551028e+01  1.526134e+02    3.078161e+01   \n",
       "\n",
       "       cat_Apps for Android  cat_Arts, Crafts & Sewing  cat_Automotive  \\\n",
       "count          9.940360e+05               9.940360e+05    9.940360e+05   \n",
       "mean           8.757220e-15               1.027800e-14    7.586651e-15   \n",
       "std            1.000001e+00               1.000001e+00    1.000001e+00   \n",
       "min           -2.016406e-02              -6.454477e-02   -7.705351e-02   \n",
       "25%           -2.016406e-02              -6.454477e-02   -7.705351e-02   \n",
       "50%           -2.016406e-02              -6.454477e-02   -7.705351e-02   \n",
       "75%           -2.016406e-02              -6.454477e-02   -7.705351e-02   \n",
       "max            4.959320e+01               1.549312e+01    1.297799e+01   \n",
       "\n",
       "           cat_Baby  cat_Baby Products      ...              pc291  \\\n",
       "count  9.940360e+05       9.940360e+05      ...       9.940360e+05   \n",
       "mean  -5.933334e-15      -1.120757e-14      ...       3.026219e-16   \n",
       "std    1.000001e+00       1.000001e+00      ...       1.000001e+00   \n",
       "min   -8.598270e-02      -2.807702e-02      ...      -6.644358e+00   \n",
       "25%   -8.598270e-02      -2.807702e-02      ...      -5.323144e-01   \n",
       "50%   -8.598270e-02      -2.807702e-02      ...      -4.044235e-03   \n",
       "75%   -8.598270e-02      -2.807702e-02      ...       5.098305e-01   \n",
       "max    1.163025e+01       3.561632e+01      ...       7.861697e+00   \n",
       "\n",
       "              pc292         pc293         pc294         pc295         pc296  \\\n",
       "count  9.940360e+05  9.940360e+05  9.940360e+05  9.940360e+05  9.940360e+05   \n",
       "mean   5.180992e-16  4.679954e-17 -2.088562e-17  2.880885e-16 -1.476404e-17   \n",
       "std    1.000001e+00  1.000001e+00  1.000001e+00  1.000001e+00  1.000001e+00   \n",
       "min   -6.188888e+00 -6.504232e+00 -6.626838e+00 -6.048934e+00 -7.119678e+00   \n",
       "25%   -5.327463e-01 -5.069046e-01 -5.008649e-01 -5.092769e-01 -4.788405e-01   \n",
       "50%   -1.560881e-03  5.647878e-03  9.869426e-04 -1.238488e-03 -2.720205e-03   \n",
       "75%    5.175950e-01  4.920822e-01  5.112307e-01  4.852234e-01  4.881347e-01   \n",
       "max    7.501955e+00  7.736336e+00  6.750965e+00  7.614505e+00  7.589073e+00   \n",
       "\n",
       "              pc297         pc298         pc299         pc300  \n",
       "count  9.940360e+05  9.940360e+05  9.940360e+05  9.940360e+05  \n",
       "mean   7.776454e-16 -2.265700e-17  3.130718e-16 -1.905817e-16  \n",
       "std    1.000001e+00  1.000001e+00  1.000001e+00  1.000001e+00  \n",
       "min   -6.373493e+00 -6.540614e+00 -6.784550e+00 -6.452385e+00  \n",
       "25%   -4.910171e-01 -4.966676e-01 -4.884351e-01 -4.919479e-01  \n",
       "50%    2.391959e-02 -9.617627e-03  8.231765e-03 -1.890337e-02  \n",
       "75%    4.884094e-01  4.719028e-01  4.680939e-01  4.490253e-01  \n",
       "max    7.803906e+00  6.582658e+00  7.929147e+00  9.044717e+00  \n",
       "\n",
       "[8 rows x 333 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## standardize numerical features\n",
    "columns = df_train.columns.drop(['reviewerID', 'asin', 'reviewTime', 'helpfulness'])\n",
    "\n",
    "## standardizing them\n",
    "scaler = StandardScaler()\n",
    "X_train = df_train[columns].copy()\n",
    "X_test = df_test[columns].copy()\n",
    "X_train[columns] = scaler.fit_transform(df_train[columns])\n",
    "X_test[columns] = scaler.transform(df_test[columns])\n",
    "\n",
    "## Split into predictors and response\n",
    "y_train = df_train.helpfulness.astype(int)\n",
    "y_test = df_test.helpfulness.astype(int)\n",
    "\n",
    "## check dimension and summary statistics\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([X_train, y_train], axis=1).to_csv('./data/df_train_final.csv.gz', encoding='utf-8', index=False, compression='gzip')\n",
    "pd.concat([X_test, y_test], axis=1).to_csv('./data/df_test_final.csv.gz', encoding='utf-8', index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>helpful_num</th>\n",
       "      <th>polarity</th>\n",
       "      <th>word_count</th>\n",
       "      <th>word_density</th>\n",
       "      <th>cat_Appliances</th>\n",
       "      <th>cat_Apps for Android</th>\n",
       "      <th>cat_Arts, Crafts &amp; Sewing</th>\n",
       "      <th>cat_Automotive</th>\n",
       "      <th>cat_Baby</th>\n",
       "      <th>cat_Baby Products</th>\n",
       "      <th>...</th>\n",
       "      <th>pc291</th>\n",
       "      <th>pc292</th>\n",
       "      <th>pc293</th>\n",
       "      <th>pc294</th>\n",
       "      <th>pc295</th>\n",
       "      <th>pc296</th>\n",
       "      <th>pc297</th>\n",
       "      <th>pc298</th>\n",
       "      <th>pc299</th>\n",
       "      <th>pc300</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.079355</td>\n",
       "      <td>0.545234</td>\n",
       "      <td>0.618807</td>\n",
       "      <td>0.407406</td>\n",
       "      <td>-0.032487</td>\n",
       "      <td>-0.020164</td>\n",
       "      <td>-0.064545</td>\n",
       "      <td>-0.077054</td>\n",
       "      <td>-0.085983</td>\n",
       "      <td>-0.028077</td>\n",
       "      <td>...</td>\n",
       "      <td>0.398561</td>\n",
       "      <td>0.446251</td>\n",
       "      <td>0.276731</td>\n",
       "      <td>1.539429</td>\n",
       "      <td>1.315107</td>\n",
       "      <td>-0.119982</td>\n",
       "      <td>0.232155</td>\n",
       "      <td>-1.606384</td>\n",
       "      <td>1.425175</td>\n",
       "      <td>0.039296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.242742</td>\n",
       "      <td>-0.901591</td>\n",
       "      <td>-0.086480</td>\n",
       "      <td>0.925713</td>\n",
       "      <td>-0.032487</td>\n",
       "      <td>-0.020164</td>\n",
       "      <td>-0.064545</td>\n",
       "      <td>-0.077054</td>\n",
       "      <td>-0.085983</td>\n",
       "      <td>-0.028077</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.532568</td>\n",
       "      <td>0.288389</td>\n",
       "      <td>0.238078</td>\n",
       "      <td>1.490016</td>\n",
       "      <td>-0.501852</td>\n",
       "      <td>-0.394231</td>\n",
       "      <td>-0.081980</td>\n",
       "      <td>0.977382</td>\n",
       "      <td>1.076143</td>\n",
       "      <td>-2.802619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.079355</td>\n",
       "      <td>-0.934753</td>\n",
       "      <td>-0.236998</td>\n",
       "      <td>-0.948043</td>\n",
       "      <td>-0.032487</td>\n",
       "      <td>-0.020164</td>\n",
       "      <td>-0.064545</td>\n",
       "      <td>-0.077054</td>\n",
       "      <td>-0.085983</td>\n",
       "      <td>-0.028077</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.194436</td>\n",
       "      <td>-0.095415</td>\n",
       "      <td>-0.268150</td>\n",
       "      <td>-0.061131</td>\n",
       "      <td>0.253660</td>\n",
       "      <td>0.046343</td>\n",
       "      <td>0.213617</td>\n",
       "      <td>-0.194900</td>\n",
       "      <td>-0.145444</td>\n",
       "      <td>-0.186946</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 333 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   helpful_num  polarity  word_count  word_density  cat_Appliances  \\\n",
       "0     0.079355  0.545234    0.618807      0.407406       -0.032487   \n",
       "1    -0.242742 -0.901591   -0.086480      0.925713       -0.032487   \n",
       "2     0.079355 -0.934753   -0.236998     -0.948043       -0.032487   \n",
       "\n",
       "   cat_Apps for Android  cat_Arts, Crafts & Sewing  cat_Automotive  cat_Baby  \\\n",
       "0             -0.020164                  -0.064545       -0.077054 -0.085983   \n",
       "1             -0.020164                  -0.064545       -0.077054 -0.085983   \n",
       "2             -0.020164                  -0.064545       -0.077054 -0.085983   \n",
       "\n",
       "   cat_Baby Products    ...        pc291     pc292     pc293     pc294  \\\n",
       "0          -0.028077    ...     0.398561  0.446251  0.276731  1.539429   \n",
       "1          -0.028077    ...    -1.532568  0.288389  0.238078  1.490016   \n",
       "2          -0.028077    ...    -0.194436 -0.095415 -0.268150 -0.061131   \n",
       "\n",
       "      pc295     pc296     pc297     pc298     pc299     pc300  \n",
       "0  1.315107 -0.119982  0.232155 -1.606384  1.425175  0.039296  \n",
       "1 -0.501852 -0.394231 -0.081980  0.977382  1.076143 -2.802619  \n",
       "2  0.253660  0.046343  0.213617 -0.194900 -0.145444 -0.186946  \n",
       "\n",
       "[3 rows x 333 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>helpful_num</th>\n",
       "      <th>polarity</th>\n",
       "      <th>word_count</th>\n",
       "      <th>word_density</th>\n",
       "      <th>cat_Appliances</th>\n",
       "      <th>cat_Apps for Android</th>\n",
       "      <th>cat_Arts, Crafts &amp; Sewing</th>\n",
       "      <th>cat_Automotive</th>\n",
       "      <th>cat_Baby</th>\n",
       "      <th>cat_Baby Products</th>\n",
       "      <th>...</th>\n",
       "      <th>pc291</th>\n",
       "      <th>pc292</th>\n",
       "      <th>pc293</th>\n",
       "      <th>pc294</th>\n",
       "      <th>pc295</th>\n",
       "      <th>pc296</th>\n",
       "      <th>pc297</th>\n",
       "      <th>pc298</th>\n",
       "      <th>pc299</th>\n",
       "      <th>pc300</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.097249</td>\n",
       "      <td>0.101297</td>\n",
       "      <td>0.184454</td>\n",
       "      <td>1.635836</td>\n",
       "      <td>-0.032487</td>\n",
       "      <td>-0.020164</td>\n",
       "      <td>-0.064545</td>\n",
       "      <td>-0.077054</td>\n",
       "      <td>-0.085983</td>\n",
       "      <td>-0.028077</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.262569</td>\n",
       "      <td>0.590528</td>\n",
       "      <td>-1.830406</td>\n",
       "      <td>0.564682</td>\n",
       "      <td>0.354593</td>\n",
       "      <td>-0.967970</td>\n",
       "      <td>0.631045</td>\n",
       "      <td>0.102797</td>\n",
       "      <td>0.637230</td>\n",
       "      <td>0.033021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.206953</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>-0.073578</td>\n",
       "      <td>0.179622</td>\n",
       "      <td>-0.032487</td>\n",
       "      <td>-0.020164</td>\n",
       "      <td>-0.064545</td>\n",
       "      <td>-0.077054</td>\n",
       "      <td>-0.085983</td>\n",
       "      <td>-0.028077</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.718051</td>\n",
       "      <td>-0.835363</td>\n",
       "      <td>1.551426</td>\n",
       "      <td>1.299062</td>\n",
       "      <td>-1.169157</td>\n",
       "      <td>0.006471</td>\n",
       "      <td>-0.304632</td>\n",
       "      <td>-0.195367</td>\n",
       "      <td>-1.000982</td>\n",
       "      <td>0.176938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.224847</td>\n",
       "      <td>-0.856343</td>\n",
       "      <td>-0.052076</td>\n",
       "      <td>-0.247294</td>\n",
       "      <td>-0.032487</td>\n",
       "      <td>-0.020164</td>\n",
       "      <td>-0.064545</td>\n",
       "      <td>-0.077054</td>\n",
       "      <td>-0.085983</td>\n",
       "      <td>-0.028077</td>\n",
       "      <td>...</td>\n",
       "      <td>0.150765</td>\n",
       "      <td>-0.409631</td>\n",
       "      <td>-1.459845</td>\n",
       "      <td>0.892071</td>\n",
       "      <td>-1.331015</td>\n",
       "      <td>0.539980</td>\n",
       "      <td>-0.622941</td>\n",
       "      <td>-0.525938</td>\n",
       "      <td>1.174542</td>\n",
       "      <td>1.241735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 333 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   helpful_num  polarity  word_count  word_density  cat_Appliances  \\\n",
       "0     0.097249  0.101297    0.184454      1.635836       -0.032487   \n",
       "1    -0.206953  0.011719   -0.073578      0.179622       -0.032487   \n",
       "2    -0.224847 -0.856343   -0.052076     -0.247294       -0.032487   \n",
       "\n",
       "   cat_Apps for Android  cat_Arts, Crafts & Sewing  cat_Automotive  cat_Baby  \\\n",
       "0             -0.020164                  -0.064545       -0.077054 -0.085983   \n",
       "1             -0.020164                  -0.064545       -0.077054 -0.085983   \n",
       "2             -0.020164                  -0.064545       -0.077054 -0.085983   \n",
       "\n",
       "   cat_Baby Products    ...        pc291     pc292     pc293     pc294  \\\n",
       "0          -0.028077    ...    -0.262569  0.590528 -1.830406  0.564682   \n",
       "1          -0.028077    ...    -0.718051 -0.835363  1.551426  1.299062   \n",
       "2          -0.028077    ...     0.150765 -0.409631 -1.459845  0.892071   \n",
       "\n",
       "      pc295     pc296     pc297     pc298     pc299     pc300  \n",
       "0  0.354593 -0.967970  0.631045  0.102797  0.637230  0.033021  \n",
       "1 -1.169157  0.006471 -0.304632 -0.195367 -1.000982  0.176938  \n",
       "2 -1.331015  0.539980 -0.622941 -0.525938  1.174542  1.241735  \n",
       "\n",
       "[3 rows x 333 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((994036, 333), (248510, 333), (994036,), (248510,))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## read dataframe from story telling procedure\n",
    "df_train = pd.read_csv('./data/df_train_final.csv.gz', compression='gzip', low_memory=False)\n",
    "df_test = pd.read_csv('./data/df_test_final.csv.gz', compression='gzip', low_memory=False)\n",
    "\n",
    "## split into X, y and train and test set\n",
    "X_train = df_train.drop(columns=['helpfulness'])\n",
    "y_train = df_train.helpfulness\n",
    "X_test = df_test.drop(columns=['helpfulness'])\n",
    "y_test = df_test.helpfulness\n",
    "\n",
    "## freeing unnecessary variables from memories\n",
    "df_train = None\n",
    "df_test = None\n",
    "gc.collect();\n",
    "\n",
    "## check \n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning frameworks\n",
    "I will consider simple, somewhat simple, complex models as candidate models. That is, I will try __Logistic Regression and Gradient Boosting frameworks and evaluate them with AP (Average Precision) score__ because we have 6 class of helpfulness ($\\in$ \\[0, 5]) and class sizes are unbalanced (skewed downward) (Reference [this paper](https://www.biostat.wisc.edu/~page/rocpr.pdf) and [scikit-learn guide](https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html#sphx-glr-auto-examples-model-selection-plot-precision-recall-py)). \n",
    "\n",
    "In information retrieval, precision is a measure of result relevancy, while recall is a measure of how many truly relevant results are returned. The precision-recall curve shows the tradeoff between precision and recall for different threshold. A high area under the curve represents both high recall and high precision, where high precision relates to a low false positive rate, and high recall relates to a low false negative rate. High scores for both show that the classifier is returning accurate results (high precision), as well as returning a majority of all positive results (high recall). \n",
    "\n",
    "<table><tr>\n",
    "    <td>\n",
    "        <figure>\n",
    "        <img src=\"https://upload.wikimedia.org/wikipedia/commons/2/26/Precisionrecall.svg\" style=\"height:600px\" align='right'>\n",
    "            <figcaption> (Precision and Recall - wikipedia) </figcaption>\n",
    "        </figure>\n",
    "    </td>\n",
    "</tr></table>\n",
    "\n",
    "Above figure is a great graphical summary of precision and recall. By definition, \n",
    "- Precision is the proportion of the data points our model says was relevant actually were relevant.\n",
    "- Recall is the ability to find all relevant instances in a dataset. \n",
    "More formally,\n",
    "\n",
    "$$ Precision = \\frac{True Positives}{True Positives + False Positives}$$\n",
    "\n",
    "$$ Recall = \\frac{True Positives}{True Positives + False Negatives}$$\n",
    "\n",
    "- Average precision (AP) summarizes a precision-recall curve. You can think of it as approximated precision-recall AUC (Area Under the Curve) score. \n",
    "\n",
    "$$ AP = \\sum_{n}(R_{n} - R_{n-1}) P_{n}$$\n",
    "Where $P_n$ and $R_n$ are the precision and recall at the $n^{th}$ threshold. Precision-recall curves are typically used in binary classification to study the output of a classifier. In order to extend the precision-recall curve and average precision to multi-class or multi-label classification, __it is necessary to binarize the output__. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use label_binarize to be multi-label setting\n",
    "Y_train = label_binarize(y_train, classes=[0, 1, 2, 3, 4, 5])\n",
    "Y_test = label_binarize(y_test, classes=[0, 1, 2, 3, 4, 5] )\n",
    "n_classes = Y_train.shape[1]\n",
    "\n",
    "## setting stratifiedKFold - 3 folds\n",
    "folds = 3\n",
    "skf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 7777)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running time for grid search with 3-fold CV: 4896 (sec)\n"
     ]
    }
   ],
   "source": [
    "## Grid Search for logistic regression\n",
    "lr = OneVsRestClassifier(LogisticRegression(random_state=1004))\n",
    "lr_params = {'estimator__C':[0.1, 0.5, 1, 5, 10]}\n",
    "\n",
    "lr_grid = GridSearchCV(lr, param_grid=lr_params,\n",
    "                    scoring='average_precision', \n",
    "                    cv=skf.split(X_train, y_train))\n",
    "\n",
    "## clock running time\n",
    "start = time.time()\n",
    "lr_grid.fit(X_train, Y_train)\n",
    "end = time.time()\n",
    "\n",
    "print(\"Running time for grid search with 3-fold CV: {:.0f} (sec)\".format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best logistic regression estimator:\n",
      "OneVsRestClassifier(estimator=LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1004, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False),\n",
      "          n_jobs=1)\n",
      "Best mean AP score for 3-fold search with 5 parameter combinations:\n",
      "0.4244511285111373\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_best = lr_grid.best_estimator_\n",
    "lr_score = lr_grid.best_score_\n",
    "lr_params = lr_grid.best_params_\n",
    "\n",
    "print(\"Best logistic regression estimator:\")\n",
    "print(lr_best)\n",
    "print(\"Best logistic regression parameters:\")\n",
    "print(lr_params)\n",
    "print(\"Best mean AP score for {}-fold grid search:\".format(folds))\n",
    "print(lr_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best logistic regression estimator AP score on test:\n",
      "0.4259\n",
      "Best logistic regression estimator AUC score on test:\n",
      "0.7667\n",
      "Best logistic regression estimator Precision score on test:\n",
      "0.6688\n",
      "Best logistic regression estimator Recall score on test:\n",
      "0.2050\n",
      "Best logistic regression estimator F1 score on test:\n",
      "0.3138\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ## AP (Average Precision) and AUC (Area Under the Curve), precision, recall, f1 scores on test data\n",
    "# lr_ap = average_precision_score(Y_test, lr_best.predict_proba(X_test))\n",
    "# lr_auc = roc_auc_score(Y_test, lr_best.predict_proba(X_test))\n",
    "# lr_precision, lr_recall, lr_f1, _ = precision_recall_fscore_support(Y_test, lr_best.predict(X_test), average='micro')\n",
    "\n",
    "# print(\"Best logistic regression estimator AP score on test:\")\n",
    "# print(\"{:.4f}\".format(lr_ap))\n",
    "# print(\"Best logistic regression estimator AUC score on test:\")\n",
    "# print(\"{:.4f}\".format(lr_aud))\n",
    "# print(\"Best logistic regression estimator Precision score on test:\")\n",
    "# print(\"{:.4f}\".format(lr_precision))\n",
    "# print(\"Best logistic regression estimator Recall score on test:\")\n",
    "# print(\"{:.4f}\".format(lr_recall))\n",
    "# print(\"Best logistic regression estimator F1 score on test:\")\n",
    "# print(\"{:.4f}\".format(lr_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## AP (Average Precision) and AUC (Area Under the Curve), precision, recall, \n",
    "## f1 scores on test data for each class\n",
    "y_lr_score = lr_best.predict_proba(X_test)\n",
    "lr_precision = dict()\n",
    "lr_recall = dict()\n",
    "lr_ap = dict()\n",
    "for i in range(n_classes):\n",
    "    lr_precision[i], lr_recall[i], _ = precision_recall_curve(Y_test[:, i],\n",
    "                                                              y_lr_score[:, i])\n",
    "    lr_ap[i] = average_precision_score(Y_test[:, i], y_lr_score[:, i])\n",
    "\n",
    "# A \"micro-average\": quantifying score on all classes jointly\n",
    "lr_precision[\"micro\"], lr_recall[\"micro\"], _ = precision_recall_curve(Y_test.ravel(),\n",
    "                                                                      y_lr_score.ravel())\n",
    "lr_ap[\"micro\"] = average_precision_score(Y_test, y_lr_score, average=\"micro\")\n",
    "lr_auc = roc_auc_score(Y_test, y_lr_score)\n",
    "lr_f1 = f1_score(Y_test, lr_best.predict(X_test), average='micro')\n",
    "lr_acc = accuracy_score(Y_test, lr_best.predict(X_test))\n",
    "\n",
    "print(\"Best logistic regression estimator micro-averaged AP score on test data:\")\n",
    "print(\"{:.4f}\".format(lr_ap[\"micro\"]))\n",
    "print(\"Best logistic regression estimator micro-averaged AUC score on test data:\")\n",
    "print(\"{:.4f}\".format(lr_auc))\n",
    "print(\"Best logistic regression estimator micro-averaged Precision score on test data:\")\n",
    "print(\"{:.4f}\".format(lr_precision[\"micro\"]))\n",
    "print(\"Best logistic regression estimator micro-averaged Recall score on test data:\")\n",
    "print(\"{:.4f}\".format(lr_recall[\"micro\"]))\n",
    "print(\"Best logistic regression estimator micro-averaged F1 score on test data:\")\n",
    "print(\"{:.4f}\".format(lr_f1))\n",
    "print(\"Best logistic regression estimator micro-averaged Accuracy score on test data:\")\n",
    "print(\"{:.4f}\".format(lr_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the precision-Recall curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "step_kwargs = ({'step': 'post'}\n",
    "               if 'step' in signature(plt.fill_between).parameters\n",
    "               else {})\n",
    "plt.step(lr_recall['micro'], lr_precision['micro'], color='b', alpha=0.2,\n",
    "         where='post')\n",
    "plt.fill_between(lr_recall[\"micro\"], lr_precision[\"micro\"], alpha=0.2, color='b',\n",
    "                 **step_kwargs)\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title(\n",
    "    'Average precision score, micro-averaged over all classes: AP={0:0.2f}'\n",
    "    .format(lr_ap[\"micro\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel SVM (RBF kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Randomized search for linear svc\n",
    "svm = SVC(random_state=330)\n",
    "svm_params = {\n",
    "            'C':[0.001, 0.01, 0.1, 1, 10],\n",
    "            'gamma':[0.001, 0.01, 0.1, 1],\n",
    "             }\n",
    "## number of parameter combinations that will be used for randomized search\n",
    "param_comb = 10 # out of 20 (5x4) comb\n",
    "svm_RS = RandomizedSearchCV(svm, param_distributions=svm_params, \n",
    "                            n_iter=param_comb, scoring='average_precision', \n",
    "                            ## run 4 jobs in parall for acceleration\n",
    "                            n_jobs=4, cv=skf.split(X_train, y_train))\n",
    "\n",
    "## clock running time\n",
    "start = time.time()\n",
    "svm_RS.fit(X_train, y_train)\n",
    "end = time.time()\n",
    "\n",
    "print(\"Running time for randomized search with 3-fold CV: {:.0f} (sec)\".format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_best = svm_RS.best_estimator_\n",
    "svm_score = svm_RS.best_score_\n",
    "svm_params = svm_RS.best_params_\n",
    "\n",
    "print(\"Best SVM estimator:\")\n",
    "print(svm_best)\n",
    "print(\"Best SVM parameters:\")\n",
    "print(svm_params)\n",
    "print(\"Best mean AP score for {}-fold search with {} parameter combinations:\".format(folds, param_comb))\n",
    "print(svm_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm_ap = average_precision_score(Y_test, y_svm_score)\n",
    "# svm_auc = roc_auc_score(Y_test, y_svm_score)\n",
    "# svm_precision, svm_recall, svm_f1, _ = precision_recall_fscore_support(Y_test, svm_best.predict(X_test), average='micro')\n",
    "\n",
    "# print(\"Best SVM estimator AP score on test:\")\n",
    "# print(\"{:.4f}\".format(svm_ap))\n",
    "# print(\"Best SVM estimator AUC score on test:\")\n",
    "# print(\"{:.4f}\".format(svm_auc))\n",
    "# print(\"Best SVM estimator Precision score on test:\")\n",
    "# print(\"{:.4f}\".format(svm_precision))\n",
    "# print(\"Best SVM estimator Recall score on test:\")\n",
    "# print(\"{:.4f}\".format(svm_recall))\n",
    "# print(\"Best SVM estimator F1 score on test:\")\n",
    "# print(\"{:.4f}\".format(svm_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.fixes import signature\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "svm_acc = accuracy_score(Y_test, svm_best.predict(X_test))\n",
    "svm_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## AP (Average Precision) and AUC (Area Under the Curve), precision, recall, \n",
    "## f1 scores on test data for each class\n",
    "y_svm_score = svm_best.decision_function(X_test)\n",
    "svm_precision = dict()\n",
    "svm_recall = dict()\n",
    "svm_ap = dict()\n",
    "for i in range(n_classes):\n",
    "    svm_precision[i], svm_recall[i], _ = precision_recall_curve(Y_test[:, i],\n",
    "                                                                y_svm_score[:, i])\n",
    "    svm_ap[i] = average_precision_score(Y_test[:, i], y_svm_score[:, i])\n",
    "\n",
    "# A \"micro-average\": quantifying score on all classes jointly\n",
    "svm_precision[\"micro\"], svm_recall[\"micro\"], _ = precision_recall_curve(Y_test.ravel(),\n",
    "                                                                        y_svm_score.ravel())\n",
    "svm_ap[\"micro\"] = average_precision_score(Y_test, y_svm_score, average=\"micro\")\n",
    "svm_auc = roc_auc_score(Y_test, y_svm_score)\n",
    "svm_f1 = f1_score(Y_test, svm_best.predict(X_test), average='micro')\n",
    "svm_acc = accuracy_score(Y_test, svm_best.predict(X_test))\n",
    "\n",
    "print(\"Best SVM estimator micro-averaged AP score on test data:\")\n",
    "print(\"{:.4f}\".format(svm_ap[\"micro\"]))\n",
    "print(\"Best SVM estimator micro-averaged AUC score on test data:\")\n",
    "print(\"{:.4f}\".format(svm_auc))\n",
    "print(\"Best SVM estimator micro-averaged Precision score on test data:\")\n",
    "print(\"{:.4f}\".format(svm_precision[\"micro\"]))\n",
    "print(\"Best SVM estimator micro-averaged Recall score on test data:\")\n",
    "print(\"{:.4f}\".format(svm_recall[\"micro\"]))\n",
    "print(\"Best SVM estimator micro-averaged F1 score on test data:\")\n",
    "print(\"{:.4f}\".format(svm_f1))\n",
    "print(\"Best SVM estimator micro-averaged Accuracy score on test data:\")\n",
    "print(\"{:.4f}\".format(svm_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the precision-Recall curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "step_kwargs = ({'step': 'post'}\n",
    "               if 'step' in signature(plt.fill_between).parameters\n",
    "               else {})\n",
    "plt.step(svm_recall['micro'], svm_precision['micro'], color='b', alpha=0.2,\n",
    "         where='post')\n",
    "plt.fill_between(svm_recall[\"micro\"], svm_precision[\"micro\"], alpha=0.2, color='b',\n",
    "                 **step_kwargs)\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title(\n",
    "    'Average precision score, micro-averaged over all classes: AP={0:0.2f}'\n",
    "    .format(svm_ap[\"micro\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = {\n",
    "            'learning_rate': [0.01, 0.05, 0.1, 0.2]\n",
    "            'min_child_weight': [1, 5, 10],\n",
    "            'gamma': [0.5, 1, 1.5, 2, 5],\n",
    "            'subsample': [0.6, 0.8, 1],\n",
    "            'colsample_bytree': [0.6, 0.8, 1],\n",
    "            'max_depth': [3, 4, 5]     \n",
    "             }\n",
    "## number of parameter that will be used for randomized search\n",
    "param_comb = 180 # out of 1620 (4x3x5x3x3x3)\n",
    "\n",
    "xgb = XGBClassifier(objective='multi:softmax', silent=True, random_state=1108)\n",
    "xgb_RS = RandomizedSearchCV(xgb, param_distributions=xgb_params, \n",
    "                            n_iter=param_comb, scoring='average_precision', \n",
    "                            ## run 2 jobs in parall for acceleration\n",
    "                            n_jobs=2, cv=skf.split(X_train, y_train))\n",
    "\n",
    "## clock running time\n",
    "start = time.time()\n",
    "xgb_RS.fit(X_train, y_train, eval_metric='map', early_stopping_rounds=10)\n",
    "end = time.time()\n",
    "\n",
    "print(\"Running time for randomized search with 3-fold CV: {:.0f} (sec)\".format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_best = xgb_RS.best_estimator_\n",
    "xgb_score = xgb_RS.best_score_\n",
    "xgb_params = xgb_RS.best_params_\n",
    "\n",
    "print(\"Best XGB estimator:\")\n",
    "print(xgb_best)\n",
    "print(\"Best XGB parameters:\")\n",
    "print(xgb_params)\n",
    "print(\"Best mean AP score for {}-fold search with {} parameter combinations:\".format(folds, param_comb))\n",
    "print(xgb_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## AP (Average Precision) and AUC (Area Under the Curve), precision, recall, f1 scores on test data\n",
    "# xgb_ap = average_precision_score(Y_test, xgb_best.predict_proba(X_test))\n",
    "# xgb_auc = roc_auc_score(Y_test, xgb_best.predict_proba(X_test))\n",
    "# xgb_precision, xgb_recall, xgb_f1, _ = precision_recall_fscore_support(Y_test, xgb_best.predict(X_test), average='micro')\n",
    "\n",
    "# print(\"Best XGB estimator AP score on test:\")\n",
    "# print(\"{:.4f}\".format(xgb_ap))\n",
    "# print(\"Best XGB estimator AUC score on test:\")\n",
    "# print(\"{:.4f}\".format(xgb_auc))\n",
    "# print(\"Best XGB estimator Precision score on test:\")\n",
    "# print(\"{:.4f}\".format(xgb_precision))\n",
    "# print(\"Best XGB estimator Recall score on test:\")\n",
    "# print(\"{:.4f}\".format(xgb_recall))\n",
    "# print(\"Best XGB estimator F1 score on test:\")\n",
    "# print(\"{:.4f}\".format(xgb_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## AP (Average Precision) and AUC (Area Under the Curve), precision, recall, \n",
    "## f1 scores on test data for each class\n",
    "y_xgb_score = xgb_best.predict_proba(X_test)\n",
    "xgb_precision = dict()\n",
    "xgb_recall = dict()\n",
    "xgb_ap = dict()\n",
    "for i in range(n_classes):\n",
    "    xgb_precision[i], xgb_recall[i], _ = precision_recall_curve(Y_test[:, i],\n",
    "                                                        y_xgb_score[:, i])\n",
    "    xgb_ap[i] = average_precision_score(Y_test[:, i], y_xgb_score[:, i])\n",
    "\n",
    "# A \"micro-average\": quantifying score on all classes jointly\n",
    "xgb_precision[\"micro\"], xgb_recall[\"micro\"], _ = precision_recall_curve(Y_test.ravel(),\n",
    "                                                                        y_xgb_score.ravel())\n",
    "xgb_ap[\"micro\"] = average_precision_score(Y_test, y_xgb_score, average=\"micro\")\n",
    "xgb_auc = roc_auc_score(Y_test, y_xgb_score)\n",
    "xgb_f1 = f1_score(Y_test, xgb_best.predict(X_test), average='micro')\n",
    "xgb_acc = accuracy_score(Y_test, xgb_best.predict(X_test))\n",
    "\n",
    "print(\"Best XGB estimator micro-averaged AP score on test data:\")\n",
    "print(\"{:.4f}\".format(xgb_ap[\"micro\"]))\n",
    "print(\"Best XGB estimator micro-averaged AUC score on test data:\")\n",
    "print(\"{:.4f}\".format(xgb_auc))\n",
    "print(\"Best XGB estimator micro-averaged Precision score on test data:\")\n",
    "print(\"{:.4f}\".format(xgb_precision[\"micro\"]))\n",
    "print(\"Best XGB estimator micro-averaged Recall score on test data:\")\n",
    "print(\"{:.4f}\".format(xgb_recall[\"micro\"]))\n",
    "print(\"Best XGB estimator micro-averaged F1 score on test data:\")\n",
    "print(\"{:.4f}\".format(xgb_f1))\n",
    "print(\"Best XGB estimator micro-averaged Accuracy score on test data:\")\n",
    "print(\"{:.4f}\".format(xgb_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the precision-Recall curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "step_kwargs = ({'step': 'post'}\n",
    "               if 'step' in signature(plt.fill_between).parameters\n",
    "               else {})\n",
    "plt.step(xgb_recall['micro'], xgb_precision['micro'], color='b', alpha=0.2,\n",
    "         where='post')\n",
    "plt.fill_between(xgb_recall[\"micro\"], xgb_precision[\"micro\"], alpha=0.2, color='b',\n",
    "                 **step_kwargs)\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title(\n",
    "    'Average precision score, micro-averaged over all classes: AP={0:0.2f}'\n",
    "    .format(xgb_ap[\"micro\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
